{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9209092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as nps\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import PIL\n",
    "\n",
    "\n",
    "from handler import geozoom_handler\n",
    "# from helpers import *\n",
    "from fc_net import *\n",
    "from utils import *\n",
    "from sa import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71afc44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 municipalities.\n"
     ]
    }
   ],
   "source": [
    "image_names = get_png_names(\"../data/MEX/\")#[0:5]\n",
    "# image_names = get_png_names(\"../pooling/data/MEX2/\")\n",
    "image_indices = random.sample(range(0, len(image_names)), 200)\n",
    "image_names = [image_names[i] for i in range(len(image_names)) if i in image_indices]\n",
    "y = get_migrants(\"../../pooling/data/migration_data.json\" , image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a7de45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18 = models.resnet18().to(device)\n",
    "attn_model = attnNet(resnet = resnet18).to(device)\n",
    "lr = .0001\n",
    "criterion = torch.nn.L1Loss(reduction = 'mean')\n",
    "attn_optimizer = torch.optim.Adam(attn_model.parameters(), lr = lr)\n",
    "\n",
    "fc_model = fc_net(resnet = resnet18).to(device)\n",
    "fc_optimizer = torch.optim.Adam(fc_model.parameters(), lr = .0001)\n",
    "\n",
    "butler = geozoom_handler(attn_model, \n",
    "                         device, \n",
    "                         criterion, \n",
    "                         attn_optimizer, \n",
    "                         fc_model = fc_model,\n",
    "                         fc_optimizer = fc_optimizer,\n",
    "                         fc_batch_size = 4,\n",
    "                         reduction_percent = .70,\n",
    "                         max_to_change = 5, \n",
    "                         plot = False, \n",
    "                         v = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64f2fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image convergence dimensions:  (62, 118)\n",
      "\n",
      "Number of training images:  120\n",
      "Number of validation images:  80\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "SPLIT = .60\n",
    "\n",
    "train_dl, val_dl = butler.prep_attn_data(image_names, y, SPLIT, BATCH_SIZE)\n",
    "\n",
    "print(\"\\nNumber of training images: \", len(train_dl))\n",
    "print(\"Number of validation images: \", len(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9467b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch               Training Loss       Validation Loss     # times clipped     # images at scale   % above max         \n",
      "Epoch: 0            2159.5928           2157.6398           0                   0                   0                   \n",
      "Epoch: 1            2152.8167           2152.6634           0                   0                   0                   \n",
      "Epoch: 2            2147.035            2143.2077           0                   0                   0                   \n",
      "Epoch: 3            2140.7318           2145.9463           0                   0                   0                   \n",
      "Epoch: 4            2134.1025           2141.13             0                   0                   0.13333333333333333 \n",
      "Epoch: 5            2127.5044           2138.0042           0                   0                   0.2916666666666667  \n",
      "Epoch: 6            2120.531            2130.1758           1                   1                   0.5333333333333333  \n",
      "Epoch: 7            2112.8258           2120.725            1                   1                   0                   \n",
      "Epoch: 8            2105.6478           2127.9594           1                   1                   0                   \n",
      "Epoch: 9            2098.949            2126.5173           1                   1                   0                   \n",
      "Epoch: 10           2092.5969           2112.2184           1                   1                   0.075               \n",
      "Epoch: 11           2086.7583           2109.279            1                   1                   0.23333333333333334 \n",
      "Epoch: 12           2080.9999           2105.3574           1                   1                   0.4                 \n",
      "Epoch: 13           2075.6521           2097.3164           2                   4                   0.5583333333333333  \n",
      "Epoch: 14           2069.4315           2094.4251           2                   4                   0                   \n"
     ]
    }
   ],
   "source": [
    "butler.train_attn_model(train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b081040",
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.go_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,o in train_dl:\n",
    "    print(load_inputs(i[0]).shape, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e984f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_im_dims(im_sizes):\n",
    "    \n",
    "#     for muni, size in im_sizes.items():\n",
    "        \n",
    "#         last_size = size[-1]\n",
    "        \n",
    "#         w, h = last_size[1] - last_size[0], last_size[3] - last_size[2]\n",
    "        \n",
    "#         print(muni, w, h)\n",
    "        \n",
    "# get_im_dims(butler.image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ac81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847163b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attn_map(impath):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to save the attention maps for a given image at each threshold\n",
    "    \"\"\"\n",
    "\n",
    "    muni_id = impath.split(\"/\")[3]\n",
    "    cur_image = load_inputs(impath)\n",
    "    folder = f\"{muni_id}_attention_maps\"\n",
    "\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "\n",
    "    for k in butler.threshold_weights.keys():\n",
    "        \n",
    "        print(k)\n",
    "\n",
    "        if k != 'fc':\n",
    "\n",
    "            model = butler.attn_model\n",
    "            model.load_state_dict(butler.threshold_weights[k])\n",
    "            model.eval()\n",
    "            IM_SIZE = (cur_image.shape[2], cur_image.shape[3])\n",
    "            gradcam, attn_heatmap = get_gradcam(model, IM_SIZE, cur_image.cuda(), target_layer = model.sa) \n",
    "            cur_image, new_dims = butler.clip_input(cur_image, attn_heatmap)\n",
    "\n",
    "            fname = os.path.join(f\"{muni_id}_attention_maps\", f\"threshold{k}_muni{muni_id}.png\") \n",
    "\n",
    "            plot_gradcam(gradcam)\n",
    "            plt.savefig(fname)\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "011fb796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "butler.plot_attn_map(impath = '../data/MEX/484031098/pngs/484031098_2010_all_box484031098_MAY.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be05612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e46ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
