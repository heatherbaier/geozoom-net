{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec41b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.models as models\n",
    "from handler import geozoom_handler\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import PIL\n",
    "\n",
    "\n",
    "from attn_model2 import *\n",
    "from fc_net import *\n",
    "from helpers import *\n",
    "from utils2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3542b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.sample(range(0, len(image_names)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357a1613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 municipalities.\n"
     ]
    }
   ],
   "source": [
    "# image_names = get_png_names(\"../pooling/data/MEX2/\")#[100:105]\n",
    "image_names = get_png_names(\"../pooling/data/MEX2/\")\n",
    "image_indices = random.sample(range(0, len(image_names)), 20)\n",
    "image_names = [image_names[i] for i in range(len(image_names)) if i in image_indices]\n",
    "y = get_migrants(\"../pooling/data/migration_data.json\" , image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e991098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1033, 1807])\n",
      "torch.Size([1, 3, 278, 610])\n",
      "torch.Size([1, 3, 1002, 872])\n",
      "torch.Size([1, 3, 965, 668])\n",
      "torch.Size([1, 3, 1251, 1823])\n",
      "torch.Size([1, 3, 1033, 687])\n",
      "torch.Size([1, 3, 125, 193])\n",
      "torch.Size([1, 3, 1674, 2084])\n",
      "torch.Size([1, 3, 260, 267])\n",
      "torch.Size([1, 3, 242, 336])\n",
      "torch.Size([1, 3, 560, 715])\n",
      "torch.Size([1, 3, 756, 847])\n",
      "torch.Size([1, 3, 977, 1185])\n",
      "torch.Size([1, 3, 1629, 1589])\n",
      "torch.Size([1, 3, 830, 1008])\n",
      "torch.Size([1, 3, 2031, 2268])\n",
      "torch.Size([1, 3, 458, 329])\n",
      "torch.Size([1, 3, 976, 1418])\n",
      "torch.Size([1, 3, 503, 459])\n",
      "torch.Size([1, 3, 419, 493])\n"
     ]
    }
   ],
   "source": [
    "for i in image_names:\n",
    "    print(load_inputs(i).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b115ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18 = models.resnet18().to(device)\n",
    "attn_model = attnNetBinary(in_channels = 512, h = 7, w = 7, batch_size = 1, resnet = resnet18).to(device)\n",
    "lr = .0001\n",
    "criterion = torch.nn.L1Loss(reduction = 'mean')\n",
    "attn_optimizer = torch.optim.Adam(attn_model.parameters(), lr = lr)\n",
    "\n",
    "fc_model = fc_net(resnet = resnet18).to(device)\n",
    "fc_optimizer = torch.optim.Adam(fc_model.parameters(), lr = .01)\n",
    "\n",
    "butler = geozoom_handler(attn_model, \n",
    "                         fc_model, device, \n",
    "                         criterion, \n",
    "                         attn_optimizer, \n",
    "                         fc_optimizer, \n",
    "                         convergence_dims = (358, 284),\n",
    "                         plot = False, \n",
    "                         v = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b499d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "SPLIT = .70\n",
    "\n",
    "train_dl, val_dl = butler.prep_attn_data(image_names, y, SPLIT, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a5b45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 260, 267]) tensor([10.], dtype=torch.float64)\n",
      "torch.Size([1, 3, 242, 336]) tensor([101.], dtype=torch.float64)\n",
      "torch.Size([1, 3, 278, 610]) tensor([1364.], dtype=torch.float64)\n",
      "torch.Size([1, 3, 976, 1418]) tensor([618.], dtype=torch.float64)\n",
      "torch.Size([1, 3, 1629, 1589]) tensor([2526.], dtype=torch.float64)\n",
      "torch.Size([1, 3, 458, 329]) tensor([8.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i,o in val_dl:\n",
    "    print(load_inputs(i[0]).shape, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e3f2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  Training Loss:  1387.1718320165362\n",
      "  Validation Loss:  772.771844069163\n",
      "Loss thresholds for training:  [0.0, 198.1674045737909, 396.3348091475818, 594.5022137213726, 792.6696182951636, 990.8370228689545, 1189.0044274427453]\n",
      "Starting from threshold:  6  with value:  1189.0044274427453\n",
      "\n",
      "\n",
      "Epoch:  1\n",
      "  Training Loss:  1304.0575648716517\n",
      "  Validation Loss:  791.1154737472534\n",
      "\n",
      "\n",
      "Epoch:  2\n",
      "  Training Loss:  1201.4100080217634\n",
      "  Validation Loss:  799.9740470250448\n",
      "\n",
      "\n",
      "Epoch:  3\n",
      "  Training Loss:  1099.6350555419922\n",
      "  Validation Loss:  790.6566088994344\n",
      "  Moving to threshold:  5   |  Next loss benchmark:  990.8370228689545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/nn/functional.py:3328: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  4\n",
      "  Training Loss:  1289.846396309989\n",
      "  Validation Loss:  932.1488068898519\n",
      "\n",
      "\n",
      "Epoch:  5\n",
      "  Training Loss:  1193.7293069022041\n",
      "  Validation Loss:  2504.8285643259683\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "  Training Loss:  1112.9698093959264\n",
      "  Validation Loss:  3798.4180183410645\n",
      "\n",
      "\n",
      "Epoch:  7\n",
      "  Training Loss:  1036.013643537249\n",
      "  Validation Loss:  2315.063376108805\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "  Training Loss:  897.5735361916678\n",
      "  Validation Loss:  6023.48117462794\n",
      "  Moving to threshold:  4   |  Next loss benchmark:  792.6696182951636\n",
      "\n",
      "\n",
      "Epoch:  9\n",
      "  Training Loss:  975.5095773424421\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "  Training Loss:  839.3008178983416\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  11\n",
      "  Training Loss:  905.5897182737078\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  12\n",
      "  Training Loss:  776.2665696144104\n",
      "  Validation Loss:  6023.48117462794\n",
      "  Moving to threshold:  3   |  Next loss benchmark:  594.5022137213726\n",
      "\n",
      "\n",
      "Epoch:  13\n",
      "  Training Loss:  1047.9971351623535\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  14\n",
      "  Training Loss:  915.9341099602835\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  15\n",
      "  Training Loss:  1055.3155615670341\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  16\n",
      "  Training Loss:  932.9285362448011\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  17\n",
      "  Training Loss:  906.7541443961007\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  18\n",
      "  Training Loss:  886.400389943804\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  19\n",
      "  Training Loss:  805.7140844890049\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  20\n",
      "  Training Loss:  795.7250394139971\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  21\n",
      "  Training Loss:  859.7131071771894\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  22\n",
      "  Training Loss:  930.0638542175293\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  23\n",
      "  Training Loss:  1029.1124395642962\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  24\n",
      "  Training Loss:  945.4992753437588\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  25\n",
      "  Training Loss:  966.8673896789551\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  26\n",
      "  Training Loss:  942.0901718139648\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  27\n",
      "  Training Loss:  904.847095489502\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  28\n",
      "  Training Loss:  983.9290365150997\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  29\n",
      "  Training Loss:  897.7123349053519\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  30\n",
      "  Training Loss:  1032.4218212536402\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  31\n",
      "  Training Loss:  960.9214027949741\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  32\n",
      "  Training Loss:  879.4371694156101\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  33\n",
      "  Training Loss:  780.5734963417053\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  34\n",
      "  Training Loss:  782.8494528702328\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  35\n",
      "  Training Loss:  801.1053784915379\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  36\n",
      "  Training Loss:  802.8182623726981\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  37\n",
      "  Training Loss:  800.653277192797\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  38\n",
      "  Training Loss:  915.8282189369202\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  39\n",
      "  Training Loss:  955.3480426583972\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  40\n",
      "  Training Loss:  835.387035710471\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  41\n",
      "  Training Loss:  890.1118477412632\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  42\n",
      "  Training Loss:  843.4955650738308\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  43\n",
      "  Training Loss:  897.8473361560276\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  44\n",
      "  Training Loss:  821.5195625168936\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  45\n",
      "  Training Loss:  1008.8902158737183\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  46\n",
      "  Training Loss:  841.2852330207825\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  47\n",
      "  Training Loss:  861.5391830035618\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  48\n",
      "  Training Loss:  841.6245715618134\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  49\n",
      "  Training Loss:  821.2280832018171\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n",
      "Epoch:  50\n",
      "  Training Loss:  808.8036506005695\n",
      "  Validation Loss:  6023.48117462794\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-107a248c0c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbutler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_attn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/rapids/notebooks/sciclone/geograd/Heather/attn/handler.py\u001b[0m in \u001b[0;36mtrain_attn_model\u001b[0;34m(self, train_dl, val_dl)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rapids/notebooks/sciclone/geograd/Heather/attn/handler.py\u001b[0m in \u001b[0;36mprep_input\u001b[0;34m(self, impath)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# Load the image as a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m# If the muni_id is in the image_sizes dictionary, clip it to the right size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \"\"\"\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "butler.train_attn_model(train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c774e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.image_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c55513",
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.threshold_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81236d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,o in train_dl:\n",
    "    print(load_inputs(i[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "641 * .70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4686f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "263 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a602b4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, o in train_dl:\n",
    "    \n",
    "    muni_id = i[0].split(\"/\")[4]\n",
    "    \n",
    "    plt.imshow(butler.prep_input(i).detach().cpu()[0].permute(1,2,0))\n",
    "    plt.title(butler.image_sizes[muni_id])\n",
    "    plt.savefig(f\"test{muni_id}.png\")\n",
    "    \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72b678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in range(50):\n",
    "# while handler.threshold_index > 0:\n",
    "\n",
    "#     for impath, output in train_dl:\n",
    "\n",
    "#         # Prep the input and pass it to the trainer (this could easily be done in one step eventually if ya want)\n",
    "#         input = handler.prep_input(impath)\n",
    "#         handler.train(input, output)\n",
    "    \n",
    "#     handler.end_epoch(train_dl, val_dl = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d17714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'484002003': (0, 224, 0, 224),\n",
       " '484016045': (0, 224, 0, 224),\n",
       " '484020489': (0, 224, 0, 224),\n",
       " '484020254': (0, 224, 0, 224)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler.image_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3e89e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(butler.threshold_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f21e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, o in val_dl:\n",
    "#     print(i, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b265c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_validation(attn_model, fc_model, input, weights_dict, plot = False):\n",
    "    \n",
    "#     muni_id = input[0].split(\"/\")[4]\n",
    "#     cur_image = load_inputs(input[0])    \n",
    "    \n",
    "#     for k in weights_dict.keys():\n",
    "                \n",
    "#         if k != 'fc':\n",
    "            \n",
    "#             model = attn_model\n",
    "#             model.load_state_dict(weights_dict[k])\n",
    "#             model.eval()\n",
    "#             IM_SIZE = (cur_image.shape[2], cur_image.shape[3])\n",
    "#             gradcam, attn_heatmap = get_gradcam(model, IM_SIZE, cur_image.cuda()) \n",
    "#             cur_image, new_dims = butler.clip_input(cur_image, attn_heatmap)\n",
    "            \n",
    "#             if plot == True:\n",
    "            \n",
    "#                 plot_gradcam(gradcam)\n",
    "#                 plt.savefig(f\"threshold{k}_muni{muni_id}.png\")\n",
    "#                 plt.clf()\n",
    "            \n",
    "        \n",
    "#         if k == 'fc':\n",
    "            \n",
    "#             model = fc_model\n",
    "#             model.load_state_dict(weights_dict[k])\n",
    "#             model.eval()\n",
    "#             return model(cur_image.cuda()).item()\n",
    "            \n",
    "\n",
    "        \n",
    "# for i, o in train_dl:\n",
    "\n",
    "#     print(round(run_validation(attn_model, fc_model, i, butler.threshold_weights), 2), \"     \", o.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfaaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
