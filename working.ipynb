{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# from utils import visualize_cam, Normalize, load_image, plot_gradcam, get_gradcam\n",
    "# from gradcam import GradCAM, GradCAMpp\n",
    "\n",
    "from attn_model2 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers import *\n",
    "# from utils import *\n",
    "\n",
    "from utils2 import *\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from handler import geozoom_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a16cd4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 municipalities.\n"
     ]
    }
   ],
   "source": [
    "image_indices = [0, 427, 775, 1000, 1042]\n",
    "\n",
    "# image_names = get_png_names(\"../pooling/data/MEX2/\")#[100:105]\n",
    "image_names = get_png_names(\"../pooling/data/MEX2/\")\n",
    "image_names = [image_names[i] for i in range(len(image_names)) if i in image_indices]\n",
    "y = get_migrants(\"../pooling/data/migration_data.json\" , image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b9d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484020535\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "x_train, y_train, x_val, y_val = train_test_split(np.array(image_names), np.array(y), .9)\n",
    "\n",
    "train = [(k,v) for k,v in zip(x_train, y_train)]\n",
    "val = [(k,v) for k,v in zip(x_val, y_val)]\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_dl = torch.utils.data.DataLoader(val, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "for i, o in val_dl:\n",
    "    print(i[0].split(\"/\")[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5a1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18 = models.resnet18().to(device)\n",
    "model = attnNetBinary(in_channels = 512, h = 7, w = 7, batch_size = 1, resnet = resnet18).to(device)\n",
    "lr = .0001\n",
    "criterion = torch.nn.L1Loss(reduction = 'mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "handler = geozoom_handler(model, device, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc0349b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  Training Loss:  1291.7023849487305\n",
      "Loss thresholds for training:  [0.0, 184.52891213553292, 369.05782427106584, 553.5867364065988, 738.1156485421317, 922.6445606776646, 1107.1734728131976]\n",
      "Starting from threshold:  6  with value:  1107.1734728131976\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  1\n",
      "  Training Loss:  1186.310401916504\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  2\n",
      "  Training Loss:  940.1547927856445\n",
      "  Updating model weights.\n",
      "  Moving to threshold:  5   |  Next loss benchmark:  922.6445606776646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/nn/functional.py:3328: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  3\n",
      "  Training Loss:  1038.580234527588\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "  Training Loss:  807.3309936523438\n",
      "  Updating model weights.\n",
      "  Moving to threshold:  4   |  Next loss benchmark:  738.1156485421317\n",
      "\n",
      "\n",
      "Epoch:  5\n",
      "  Training Loss:  1087.2178039550781\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "  Training Loss:  902.3010330200195\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  7\n",
      "  Training Loss:  1092.048210144043\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "  Training Loss:  823.0031280517578\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  9\n",
      "  Training Loss:  745.8505859375\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "  Training Loss:  865.6685791015625\n",
      "\n",
      "\n",
      "Epoch:  11\n",
      "  Training Loss:  735.8683471679688\n",
      "  Updating model weights.\n",
      "  Moving to threshold:  3   |  Next loss benchmark:  553.5867364065988\n",
      "\n",
      "\n",
      "Epoch:  12\n",
      "  Training Loss:  829.1408538818359\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  13\n",
      "  Training Loss:  848.6707534790039\n",
      "\n",
      "\n",
      "Epoch:  14\n",
      "  Training Loss:  837.9545364379883\n",
      "\n",
      "\n",
      "Epoch:  15\n",
      "  Training Loss:  758.9454116821289\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  16\n",
      "  Training Loss:  785.0429840087891\n",
      "\n",
      "\n",
      "Epoch:  17\n",
      "  Training Loss:  748.6486587524414\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  18\n",
      "  Training Loss:  839.1218414306641\n",
      "\n",
      "\n",
      "Epoch:  19\n",
      "  Training Loss:  811.6604537963867\n",
      "\n",
      "\n",
      "Epoch:  20\n",
      "  Training Loss:  779.8333892822266\n",
      "\n",
      "\n",
      "Epoch:  21\n",
      "  Training Loss:  845.150749206543\n",
      "\n",
      "\n",
      "Epoch:  22\n",
      "  Training Loss:  740.3069229125977\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  23\n",
      "  Training Loss:  803.8459968566895\n",
      "\n",
      "\n",
      "Epoch:  24\n",
      "  Training Loss:  773.8788681030273\n",
      "\n",
      "\n",
      "Epoch:  25\n",
      "  Training Loss:  847.9780731201172\n",
      "\n",
      "\n",
      "Epoch:  26\n",
      "  Training Loss:  972.4400329589844\n",
      "\n",
      "\n",
      "Epoch:  27\n",
      "  Training Loss:  765.8041534423828\n",
      "\n",
      "\n",
      "Epoch:  28\n",
      "  Training Loss:  727.7945785522461\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  29\n",
      "  Training Loss:  713.3055419921875\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  30\n",
      "  Training Loss:  677.4780349731445\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  31\n",
      "  Training Loss:  679.5880661010742\n",
      "\n",
      "\n",
      "Epoch:  32\n",
      "  Training Loss:  735.4286727905273\n",
      "\n",
      "\n",
      "Epoch:  33\n",
      "  Training Loss:  702.2560882568359\n",
      "\n",
      "\n",
      "Epoch:  34\n",
      "  Training Loss:  651.6103210449219\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  35\n",
      "  Training Loss:  636.7782440185547\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  36\n",
      "  Training Loss:  650.0665283203125\n",
      "\n",
      "\n",
      "Epoch:  37\n",
      "  Training Loss:  677.413703918457\n",
      "\n",
      "\n",
      "Epoch:  38\n",
      "  Training Loss:  859.6489105224609\n",
      "\n",
      "\n",
      "Epoch:  39\n",
      "  Training Loss:  705.4632263183594\n",
      "\n",
      "\n",
      "Epoch:  40\n",
      "  Training Loss:  537.7902603149414\n",
      "  Updating model weights.\n",
      "  Moving to threshold:  2   |  Next loss benchmark:  369.05782427106584\n",
      "\n",
      "\n",
      "Epoch:  41\n",
      "  Training Loss:  671.0042037963867\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  42\n",
      "  Training Loss:  759.8743667602539\n",
      "\n",
      "\n",
      "Epoch:  43\n",
      "  Training Loss:  680.9125747680664\n",
      "\n",
      "\n",
      "Epoch:  44\n",
      "  Training Loss:  617.7698211669922\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  45\n",
      "  Training Loss:  698.3365325927734\n",
      "\n",
      "\n",
      "Epoch:  46\n",
      "  Training Loss:  546.230827331543\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  47\n",
      "  Training Loss:  735.4168243408203\n",
      "\n",
      "\n",
      "Epoch:  48\n",
      "  Training Loss:  520.8646774291992\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  49\n",
      "  Training Loss:  574.5885162353516\n",
      "\n",
      "\n",
      "Epoch:  50\n",
      "  Training Loss:  943.4180450439453\n",
      "\n",
      "\n",
      "Epoch:  51\n",
      "  Training Loss:  740.0824203491211\n",
      "\n",
      "\n",
      "Epoch:  52\n",
      "  Training Loss:  344.1143608093262\n",
      "  Updating model weights.\n",
      "  Moving to threshold:  1   |  Next loss benchmark:  184.52891213553292\n",
      "\n",
      "\n",
      "Epoch:  53\n",
      "  Training Loss:  775.4284439086914\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  54\n",
      "  Training Loss:  606.452751159668\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  55\n",
      "  Training Loss:  626.2214813232422\n",
      "\n",
      "\n",
      "Epoch:  56\n",
      "  Training Loss:  600.2692718505859\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  57\n",
      "  Training Loss:  564.4404296875\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  58\n",
      "  Training Loss:  573.6042633056641\n",
      "\n",
      "\n",
      "Epoch:  59\n",
      "  Training Loss:  366.47852325439453\n",
      "  Updating model weights.\n",
      "\n",
      "\n",
      "Epoch:  60\n",
      "  Training Loss:  177.70449829101562\n",
      "  Updating model weights.\n",
      "  Moving to threshold:  0   |  Next loss benchmark:  0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for i in range(50):\n",
    "while handler.threshold_index > 0:\n",
    "\n",
    "    for impath, output in train_dl:\n",
    "\n",
    "        # Prep the input and pass it to the trainer (this could easily be done in one step eventually if ya want)\n",
    "        input = handler.prep_input(impath)\n",
    "        handler.train(input, output)\n",
    "    \n",
    "    handler.end_epoch(train_dl, val_dl = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22336ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'484002003': (0, 224, 0, 224),\n",
       " '484016045': (0, 224, 0, 224),\n",
       " '484020489': (0, 224, 0, 224),\n",
       " '484020254': (0, 224, 0, 224)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler.image_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43742cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([6, 5, 4, 3, 2, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler.threshold_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f2c069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('../pooling/data/MEX2/484020535/pngs/484020535_2010_1_box484020535_MAY.png',) tensor([89.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i, o in val_dl:\n",
    "    print(i, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b5b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4e737eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting shape:  torch.Size([1, 3, 643, 457])\n",
      "torch.Size([1, 3, 450, 319])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "175.85       334.0\n",
      "starting shape:  torch.Size([1, 3, 423, 628])\n",
      "torch.Size([1, 3, 296, 439])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "142.18       1352.0\n",
      "starting shape:  torch.Size([1, 3, 562, 981])\n",
      "torch.Size([1, 3, 393, 686])\n",
      "torch.Size([1, 3, 275, 480])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "2505666.0       348.0\n",
      "starting shape:  torch.Size([1, 3, 1488, 3072])\n",
      "torch.Size([1, 3, 1041, 2150])\n",
      "torch.Size([1, 3, 728, 1505])\n",
      "torch.Size([1, 3, 509, 1053])\n",
      "torch.Size([1, 3, 356, 737])\n",
      "torch.Size([1, 3, 249, 515])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "56354.76       3162.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_validation(model, input, weights_dict):\n",
    "    \n",
    "    muni_id = input[0].split(\"/\")[4]\n",
    "    cur_image = load_inputs(input[0])\n",
    "    \n",
    "    print(\"starting shape: \", cur_image.shape)\n",
    "    \n",
    "    \n",
    "    for k in weights_dict.keys():\n",
    "        \n",
    "        model.load_state_dict(weights_dict[k])\n",
    "        model.eval()\n",
    "        IM_SIZE = (cur_image.shape[2], cur_image.shape[3])\n",
    "        gradcam, attn_heatmap = get_gradcam(model, IM_SIZE, cur_image.cuda()) \n",
    "        cur_image, new_dims = handler.clip_input(cur_image, attn_heatmap)\n",
    "        print(cur_image.shape)\n",
    "        plot_gradcam(gradcam)\n",
    "        plt.savefig(f\"threshold{k}_muni{muni_id}.png\")\n",
    "        plt.clf()\n",
    "\n",
    "        if k == 1:\n",
    "            \n",
    "            return model(cur_image.cuda()).item()\n",
    "        \n",
    "        \n",
    "for i, o in train_dl:\n",
    "\n",
    "    print(round(run_validation(model, i, handler.threshold_weights), 2), \"     \", o.item())\n",
    "    \n",
    "#     print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b072a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
