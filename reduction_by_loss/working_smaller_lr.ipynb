{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef758fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import PIL\n",
    "\n",
    "\n",
    "from handler import geozoom_handler\n",
    "# from helpers import *\n",
    "from fc_net import *\n",
    "from utils import *\n",
    "from sa import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b976f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.sample(range(0, len(image_names)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e26612c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 municipalities.\n"
     ]
    }
   ],
   "source": [
    "# image_names = get_png_names(\"../pooling/data/MEX2/\")#[100:105]\n",
    "image_names = get_png_names(\"../pooling/data/MEX2/\")\n",
    "image_indices = random.sample(range(0, len(image_names)), 1000)\n",
    "image_names = [image_names[i] for i in range(len(image_names)) if i in image_indices]\n",
    "y = get_migrants(\"../pooling/data/migration_data.json\" , image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "993059de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in image_names:\n",
    "#     print(load_inputs(i).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d63952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18 = models.resnet18().to(device)\n",
    "attn_model = attnNet(resnet = resnet18).to(device)\n",
    "lr = .0001\n",
    "criterion = torch.nn.L1Loss(reduction = 'mean')\n",
    "attn_optimizer = torch.optim.Adam(attn_model.parameters(), lr = lr)\n",
    "\n",
    "fc_model = fc_net(resnet = resnet18).to(device)\n",
    "fc_optimizer = torch.optim.Adam(fc_model.parameters(), lr = .01)\n",
    "\n",
    "butler = geozoom_handler(attn_model, \n",
    "                         fc_model, device, \n",
    "                         criterion, \n",
    "                         attn_optimizer, \n",
    "                         fc_optimizer, \n",
    "                         num_thresholds = 10,\n",
    "                         reduction_percent = .90,\n",
    "                         convergence_dims = (358, 284),\n",
    "                         plot = False, \n",
    "                         v = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa00859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (query_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (key_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (value_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_model.sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b76a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "SPLIT = .60\n",
    "\n",
    "train_dl, val_dl = butler.prep_attn_data(image_names, y, SPLIT, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d357c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,o in val_dl:\n",
    "#     print(load_inputs(i[0]).shape, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d7910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  Training Loss:  1546.095612830321\n",
      "  Validation Loss:  1810.2818915796279\n",
      "Loss thresholds for training:  [0.0, 181.0281891579628, 362.0563783159256, 543.0845674738885, 724.1127566318512, 905.1409457898139, 1086.169134947777, 1267.1973241057397, 1448.2255132637024, 1629.2537024216651]\n",
      "Starting from threshold:  9  with value:  1629.2537024216651\n",
      "\n",
      "\n",
      "Epoch:  1\n",
      "  Training Loss:  1514.9631309318543\n",
      "  Validation Loss:  1788.1201807022094\n",
      "\n",
      "\n",
      "Epoch:  2\n",
      "  Training Loss:  1490.8418667348226\n",
      "  Validation Loss:  1745.2964589118958\n",
      "\n",
      "\n",
      "Epoch:  3\n",
      "  Training Loss:  1474.1223415120442\n",
      "  Validation Loss:  1692.3441275215148\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "  Training Loss:  1463.1278290812174\n",
      "  Validation Loss:  1702.7102558135987\n",
      "\n",
      "\n",
      "Epoch:  5\n",
      "  Training Loss:  1451.9288108317057\n",
      "  Validation Loss:  1701.4822577667237\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "  Training Loss:  1445.3556198120118\n",
      "  Validation Loss:  1705.7213681411743\n",
      "\n",
      "\n",
      "Epoch:  7\n",
      "  Training Loss:  1437.6554746246338\n",
      "  Validation Loss:  1695.939594230652\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "  Training Loss:  1430.3566351572672\n",
      "  Validation Loss:  1701.2131749725343\n",
      "\n",
      "\n",
      "Epoch:  9\n",
      "  Training Loss:  1426.471092936198\n",
      "  Validation Loss:  1679.9348461151124\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "  Training Loss:  1421.057575937907\n",
      "  Validation Loss:  1662.040244178772\n",
      "\n",
      "\n",
      "Epoch:  11\n",
      "  Training Loss:  1419.2655509948731\n",
      "  Validation Loss:  1661.9837970733643\n",
      "\n",
      "\n",
      "Epoch:  12\n",
      "  Training Loss:  1414.0108335367838\n",
      "  Validation Loss:  1663.1962156677246\n",
      "\n",
      "\n",
      "Epoch:  13\n",
      "  Training Loss:  1407.5649852498373\n",
      "  Validation Loss:  1655.3930842590332\n",
      "\n",
      "\n",
      "Epoch:  14\n",
      "  Training Loss:  1401.281151529948\n",
      "  Validation Loss:  1658.5186457824707\n",
      "\n",
      "\n",
      "Epoch:  15\n",
      "  Training Loss:  1398.9735837809244\n",
      "  Validation Loss:  1706.857130355835\n",
      "\n",
      "\n",
      "Epoch:  16\n",
      "  Training Loss:  1394.1334308624268\n",
      "  Validation Loss:  1678.7009494018555\n",
      "\n",
      "\n",
      "Epoch:  17\n",
      "  Training Loss:  1385.8151502227784\n",
      "  Validation Loss:  1666.4958199691773\n",
      "\n",
      "\n",
      "Epoch:  18\n",
      "  Training Loss:  1386.0056271107992\n",
      "  Validation Loss:  1656.197448387146\n",
      "\n",
      "\n",
      "Epoch:  19\n",
      "  Training Loss:  1380.9038907877605\n",
      "  Validation Loss:  1648.1068199539184\n",
      "\n",
      "\n",
      "Epoch:  20\n",
      "  Training Loss:  1376.9065451049805\n",
      "  Validation Loss:  1670.3174370956422\n",
      "\n",
      "\n",
      "Epoch:  21\n",
      "  Training Loss:  1376.859290898641\n",
      "  Validation Loss:  1687.3479992675782\n",
      "\n",
      "\n",
      "Epoch:  22\n",
      "  Training Loss:  1382.8212389628093\n",
      "  Validation Loss:  1626.0096157836915\n",
      "  Moving to threshold:  8   |  Next loss benchmark:  1448.2255132637024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/nn/functional.py:3328: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  23\n",
      "  Training Loss:  1371.32966120402\n",
      "  Validation Loss:  1663.2698483276367\n",
      "\n",
      "\n",
      "Epoch:  24\n",
      "  Training Loss:  1367.5335594177245\n",
      "  Validation Loss:  1648.7203842926026\n",
      "\n",
      "\n",
      "Epoch:  25\n",
      "  Training Loss:  1368.045519205729\n",
      "  Validation Loss:  1652.5007053375243\n",
      "\n",
      "\n",
      "Epoch:  26\n",
      "  Training Loss:  1362.9273852030435\n",
      "  Validation Loss:  1660.7656249237061\n",
      "\n",
      "\n",
      "Epoch:  27\n",
      "  Training Loss:  1363.3640367635091\n",
      "  Validation Loss:  1629.2765606689454\n",
      "\n",
      "\n",
      "Epoch:  28\n",
      "  Training Loss:  1361.2716336568196\n",
      "  Validation Loss:  1629.1995183563233\n",
      "\n",
      "\n",
      "Epoch:  29\n",
      "  Training Loss:  1355.0506507873536\n",
      "  Validation Loss:  1625.2450507354736\n",
      "\n",
      "\n",
      "Epoch:  30\n",
      "  Training Loss:  1352.7777104187012\n",
      "  Validation Loss:  1663.035464401245\n",
      "\n",
      "\n",
      "Epoch:  31\n",
      "  Training Loss:  1352.802134806315\n",
      "  Validation Loss:  1642.6272906494141\n",
      "\n",
      "\n",
      "Epoch:  32\n",
      "  Training Loss:  1354.1142830403646\n",
      "  Validation Loss:  1643.9824560546874\n",
      "\n",
      "\n",
      "Epoch:  33\n",
      "  Training Loss:  1353.1140079243978\n",
      "  Validation Loss:  1626.5539530181884\n",
      "\n",
      "\n",
      "Epoch:  34\n",
      "  Training Loss:  1357.9013002522786\n",
      "  Validation Loss:  1650.3003860473632\n",
      "\n",
      "\n",
      "Epoch:  35\n",
      "  Training Loss:  1350.4495917256672\n",
      "  Validation Loss:  1646.2949928283692\n",
      "\n",
      "\n",
      "Epoch:  36\n",
      "  Training Loss:  1348.001647644043\n",
      "  Validation Loss:  1619.815449295044\n",
      "\n",
      "\n",
      "Epoch:  37\n",
      "  Training Loss:  1342.6563898213703\n",
      "  Validation Loss:  1649.692319946289\n",
      "\n",
      "\n",
      "Epoch:  38\n",
      "  Training Loss:  1341.9904404703775\n",
      "  Validation Loss:  1617.5618686676025\n",
      "\n",
      "\n",
      "Epoch:  39\n",
      "  Training Loss:  1340.1507602945965\n",
      "  Validation Loss:  1619.8819357299806\n",
      "\n",
      "\n",
      "Epoch:  40\n",
      "  Training Loss:  1338.3459984842937\n",
      "  Validation Loss:  1635.4246200561524\n",
      "\n",
      "\n",
      "Epoch:  41\n",
      "  Training Loss:  1332.0360544840494\n",
      "  Validation Loss:  1651.5765937042236\n",
      "\n",
      "\n",
      "Epoch:  42\n",
      "  Training Loss:  1332.788818206787\n",
      "  Validation Loss:  1623.5629782104493\n",
      "\n",
      "\n",
      "Epoch:  43\n",
      "  Training Loss:  1335.3400509134929\n",
      "  Validation Loss:  1677.1755693817138\n",
      "\n",
      "\n",
      "Epoch:  44\n",
      "  Training Loss:  1358.72828414917\n",
      "  Validation Loss:  1679.2072802734374\n",
      "\n",
      "\n",
      "Epoch:  45\n",
      "  Training Loss:  1350.8558352661132\n",
      "  Validation Loss:  1888.7180030059815\n",
      "\n",
      "\n",
      "Epoch:  46\n",
      "  Training Loss:  1355.7722368367513\n",
      "  Validation Loss:  1650.0388345336914\n",
      "\n",
      "\n",
      "Epoch:  47\n",
      "  Training Loss:  1346.164131571452\n",
      "  Validation Loss:  1646.076613998413\n",
      "\n",
      "\n",
      "Epoch:  48\n",
      "  Training Loss:  1343.8530832417805\n",
      "  Validation Loss:  1652.109820098877\n",
      "\n",
      "\n",
      "Epoch:  49\n",
      "  Training Loss:  1340.270153910319\n",
      "  Validation Loss:  1649.8297827911376\n",
      "\n",
      "\n",
      "Epoch:  50\n",
      "  Training Loss:  1335.6831926472983\n",
      "  Validation Loss:  1670.0837384796143\n",
      "\n",
      "\n",
      "Epoch:  51\n",
      "  Training Loss:  1335.5908826700847\n",
      "  Validation Loss:  1654.6313556671143\n",
      "\n",
      "\n",
      "Epoch:  52\n",
      "  Training Loss:  1326.7287245178222\n",
      "  Validation Loss:  1683.7423837280273\n",
      "\n",
      "\n",
      "Epoch:  53\n",
      "  Training Loss:  1328.0112331644693\n",
      "  Validation Loss:  1653.5776483917236\n",
      "\n",
      "\n",
      "Epoch:  54\n",
      "  Training Loss:  1331.0949441019693\n",
      "  Validation Loss:  1658.8274662780761\n",
      "\n",
      "\n",
      "Epoch:  55\n",
      "  Training Loss:  1330.06533106486\n",
      "  Validation Loss:  1628.416072998047\n",
      "\n",
      "\n",
      "Epoch:  56\n",
      "  Training Loss:  1332.216474863688\n",
      "  Validation Loss:  1649.103341140747\n",
      "\n",
      "\n",
      "Epoch:  57\n",
      "  Training Loss:  1333.2986855570475\n",
      "  Validation Loss:  1773.9020091247558\n",
      "\n",
      "\n",
      "Epoch:  58\n",
      "  Training Loss:  1338.6938494873048\n",
      "  Validation Loss:  1621.3696927642823\n",
      "\n",
      "\n",
      "Epoch:  59\n",
      "  Training Loss:  1329.0369138590495\n",
      "  Validation Loss:  1633.5574997711183\n",
      "\n",
      "\n",
      "Epoch:  60\n",
      "  Training Loss:  1319.221209716797\n",
      "  Validation Loss:  1632.1708110046386\n",
      "\n",
      "\n",
      "Epoch:  61\n",
      "  Training Loss:  1322.419539489746\n",
      "  Validation Loss:  1645.1753718566895\n",
      "\n",
      "\n",
      "Epoch:  62\n",
      "  Training Loss:  1317.3909852600098\n",
      "  Validation Loss:  1817.4412693786621\n",
      "\n",
      "\n",
      "Epoch:  63\n",
      "  Training Loss:  1314.2577750142416\n",
      "  Validation Loss:  1638.1545274734497\n",
      "\n",
      "\n",
      "Epoch:  64\n",
      "  Training Loss:  1313.3832687886556\n",
      "  Validation Loss:  1614.0849952697754\n",
      "\n",
      "\n",
      "Epoch:  65\n",
      "  Training Loss:  1314.773361155192\n",
      "  Validation Loss:  1617.7045164489746\n",
      "\n",
      "\n",
      "Epoch:  66\n",
      "  Training Loss:  1310.4664302062988\n",
      "  Validation Loss:  1627.594085998535\n",
      "\n",
      "\n",
      "Epoch:  67\n",
      "  Training Loss:  1310.1954090372722\n",
      "  Validation Loss:  1634.6601133728027\n",
      "\n",
      "\n",
      "Epoch:  68\n",
      "  Training Loss:  1307.5047222391765\n",
      "  Validation Loss:  1637.2649472045898\n",
      "\n",
      "\n",
      "Epoch:  69\n",
      "  Training Loss:  1310.021778640747\n",
      "  Validation Loss:  1632.2406239318848\n",
      "\n",
      "\n",
      "Epoch:  70\n",
      "  Training Loss:  1306.707469100952\n",
      "  Validation Loss:  1630.818525466919\n",
      "\n",
      "\n",
      "Epoch:  71\n",
      "  Training Loss:  1293.7782893625895\n",
      "  Validation Loss:  1628.4244596099854\n",
      "\n",
      "\n",
      "Epoch:  72\n",
      "  Training Loss:  1302.0153419748942\n",
      "  Validation Loss:  1607.9468340301514\n",
      "\n",
      "\n",
      "Epoch:  73\n",
      "  Training Loss:  1310.9371270751953\n",
      "  Validation Loss:  1630.5355383682252\n",
      "\n",
      "\n",
      "Epoch:  74\n",
      "  Training Loss:  1308.9390900675455\n",
      "  Validation Loss:  1649.3587447738648\n",
      "\n",
      "\n",
      "Epoch:  75\n",
      "  Training Loss:  1319.9724115753174\n",
      "  Validation Loss:  1678.6603676223756\n",
      "\n",
      "\n",
      "Epoch:  76\n",
      "  Training Loss:  1321.1030504099529\n",
      "  Validation Loss:  1653.1939046096802\n",
      "\n",
      "\n",
      "Epoch:  77\n",
      "  Training Loss:  1304.3332346343993\n",
      "  Validation Loss:  1612.8266271209716\n",
      "\n",
      "\n",
      "Epoch:  78\n",
      "  Training Loss:  1304.8194222513835\n",
      "  Validation Loss:  1634.5114276885986\n",
      "\n",
      "\n",
      "Epoch:  79\n",
      "  Training Loss:  1302.8001633453368\n",
      "  Validation Loss:  1947.6475706481933\n",
      "\n",
      "\n",
      "Epoch:  80\n",
      "  Training Loss:  1293.930180638631\n",
      "  Validation Loss:  1626.5542720794679\n",
      "\n",
      "\n",
      "Epoch:  81\n",
      "  Training Loss:  1300.8218210601806\n",
      "  Validation Loss:  1670.9070248031617\n",
      "\n",
      "\n",
      "Epoch:  82\n",
      "  Training Loss:  1293.7677497609457\n",
      "  Validation Loss:  1629.9542032623292\n",
      "\n",
      "\n",
      "Epoch:  83\n",
      "  Training Loss:  1305.3171739959716\n",
      "  Validation Loss:  1668.2339651489258\n",
      "\n",
      "\n",
      "Epoch:  84\n",
      "  Training Loss:  1296.1265148671469\n",
      "  Validation Loss:  1658.5609479141235\n",
      "\n",
      "\n",
      "Epoch:  85\n",
      "  Training Loss:  1303.5015467071532\n",
      "  Validation Loss:  1680.8266143417359\n",
      "\n",
      "\n",
      "Epoch:  86\n",
      "  Training Loss:  1296.7696336364745\n",
      "  Validation Loss:  1642.0639350891113\n",
      "\n",
      "\n",
      "Epoch:  87\n",
      "  Training Loss:  1299.262214457194\n",
      "  Validation Loss:  1631.1169828033446\n",
      "\n",
      "\n",
      "Epoch:  88\n",
      "  Training Loss:  1318.9369269816082\n",
      "  Validation Loss:  1806.24185836792\n",
      "\n",
      "\n",
      "Epoch:  89\n",
      "  Training Loss:  1306.402122929891\n",
      "  Validation Loss:  1656.779429244995\n",
      "\n",
      "\n",
      "Epoch:  90\n",
      "  Training Loss:  1297.968645655314\n",
      "  Validation Loss:  1645.7739442062377\n",
      "\n",
      "\n",
      "Epoch:  91\n",
      "  Training Loss:  1295.9823822275798\n",
      "  Validation Loss:  1801.242798728943\n",
      "\n",
      "\n",
      "Epoch:  92\n",
      "  Training Loss:  1307.3144764200847\n",
      "  Validation Loss:  1654.8437781143189\n",
      "\n",
      "\n",
      "Epoch:  93\n",
      "  Training Loss:  1295.7765797424317\n",
      "  Validation Loss:  1646.657708053589\n",
      "\n",
      "\n",
      "Epoch:  94\n",
      "  Training Loss:  1294.2657748158772\n",
      "  Validation Loss:  1623.0878818511962\n",
      "\n",
      "\n",
      "Epoch:  95\n",
      "  Training Loss:  1295.1442466990154\n",
      "  Validation Loss:  1677.315994720459\n",
      "\n",
      "\n",
      "Epoch:  96\n",
      "  Training Loss:  1290.5538489023845\n",
      "  Validation Loss:  1619.8470394134522\n",
      "\n",
      "\n",
      "Epoch:  97\n",
      "  Training Loss:  1283.9983862559\n",
      "  Validation Loss:  1652.0585595703126\n",
      "\n",
      "\n",
      "Epoch:  98\n",
      "  Training Loss:  1297.6471812438965\n",
      "  Validation Loss:  1721.5908840179443\n",
      "\n",
      "\n",
      "Epoch:  99\n",
      "  Training Loss:  1283.7181071217856\n",
      "  Validation Loss:  1675.9486792373657\n",
      "\n",
      "\n",
      "Epoch:  100\n",
      "  Training Loss:  1292.8378899637858\n",
      "  Validation Loss:  1623.9899635314941\n",
      "\n",
      "\n",
      "Epoch:  101\n",
      "  Training Loss:  1287.7259497070313\n",
      "  Validation Loss:  1630.1494828033447\n",
      "\n",
      "\n",
      "Epoch:  102\n",
      "  Training Loss:  1284.0923893737793\n",
      "  Validation Loss:  1650.248108139038\n",
      "\n",
      "\n",
      "Epoch:  103\n",
      "  Training Loss:  1282.4355433146159\n",
      "  Validation Loss:  1627.7740167999268\n",
      "\n",
      "\n",
      "Epoch:  104\n",
      "  Training Loss:  1273.9458754221598\n",
      "  Validation Loss:  1633.29157371521\n",
      "\n",
      "\n",
      "Epoch:  105\n",
      "  Training Loss:  1267.155556971232\n",
      "  Validation Loss:  1633.3523613739014\n",
      "\n",
      "\n",
      "Epoch:  106\n",
      "  Training Loss:  1264.978988571167\n",
      "  Validation Loss:  1669.7164632415772\n",
      "\n",
      "\n",
      "Epoch:  107\n",
      "  Training Loss:  1265.4989215342205\n",
      "  Validation Loss:  1691.1126222991943\n",
      "\n",
      "\n",
      "Epoch:  108\n",
      "  Training Loss:  1271.166462020874\n",
      "  Validation Loss:  1647.211076965332\n",
      "\n",
      "\n",
      "Epoch:  109\n",
      "  Training Loss:  1267.8407634989421\n",
      "  Validation Loss:  1708.1205463600159\n",
      "\n",
      "\n",
      "Epoch:  110\n",
      "  Training Loss:  1267.0759377034506\n",
      "  Validation Loss:  1664.7844357299805\n",
      "\n",
      "\n",
      "Epoch:  111\n",
      "  Training Loss:  1271.0244045003255\n",
      "  Validation Loss:  1668.8841401672364\n",
      "\n",
      "\n",
      "Epoch:  112\n",
      "  Training Loss:  1277.2037790934244\n",
      "  Validation Loss:  1641.2759700775146\n",
      "\n",
      "\n",
      "Epoch:  113\n",
      "  Training Loss:  1261.135676015218\n",
      "  Validation Loss:  1651.1888990020752\n",
      "\n",
      "\n",
      "Epoch:  114\n",
      "  Training Loss:  1263.9390175882975\n",
      "  Validation Loss:  1677.6748790359497\n",
      "\n",
      "\n",
      "Epoch:  115\n",
      "  Training Loss:  1258.6118475087483\n",
      "  Validation Loss:  1649.9010619354249\n",
      "\n",
      "\n",
      "Epoch:  116\n",
      "  Training Loss:  1256.702494837443\n",
      "  Validation Loss:  1686.3463981628418\n",
      "\n",
      "\n",
      "Epoch:  117\n",
      "  Training Loss:  1232.833102722168\n",
      "  Validation Loss:  1694.9330018615722\n",
      "\n",
      "\n",
      "Epoch:  118\n",
      "  Training Loss:  1281.7176990000407\n",
      "  Validation Loss:  1737.2190564727782\n",
      "\n",
      "\n",
      "Epoch:  119\n",
      "  Training Loss:  1292.4460226694744\n",
      "  Validation Loss:  1710.359775466919\n",
      "\n",
      "\n",
      "Epoch:  120\n",
      "  Training Loss:  1290.0338624064127\n",
      "  Validation Loss:  1690.2571009445192\n",
      "\n",
      "\n",
      "Epoch:  121\n",
      "  Training Loss:  1273.4661420694988\n",
      "  Validation Loss:  1697.8210584259034\n",
      "\n",
      "\n",
      "Epoch:  122\n",
      "  Training Loss:  1277.0580694580078\n",
      "  Validation Loss:  1656.2550036621094\n",
      "\n",
      "\n",
      "Epoch:  123\n",
      "  Training Loss:  1266.9410912577312\n",
      "  Validation Loss:  1623.2395568847655\n",
      "\n",
      "\n",
      "Epoch:  124\n",
      "  Training Loss:  1271.2143922678629\n",
      "  Validation Loss:  1660.7817413711548\n",
      "\n",
      "\n",
      "Epoch:  125\n",
      "  Training Loss:  1282.4292100524901\n",
      "  Validation Loss:  1689.879429550171\n",
      "\n",
      "\n",
      "Epoch:  126\n",
      "  Training Loss:  1262.4169583129883\n",
      "  Validation Loss:  1625.1255924224854\n",
      "\n",
      "\n",
      "Epoch:  127\n",
      "  Training Loss:  1259.2123132832844\n",
      "  Validation Loss:  1659.3039811706542\n",
      "\n",
      "\n",
      "Epoch:  128\n",
      "  Training Loss:  1250.4545761617026\n",
      "  Validation Loss:  1643.5514025115967\n",
      "\n",
      "\n",
      "Epoch:  129\n",
      "  Training Loss:  1256.8665848032633\n",
      "  Validation Loss:  1638.863647994995\n",
      "\n",
      "\n",
      "Epoch:  130\n",
      "  Training Loss:  1272.615524520874\n",
      "  Validation Loss:  1622.3975861358642\n",
      "\n",
      "\n",
      "Epoch:  131\n",
      "  Training Loss:  1268.6569647216797\n",
      "  Validation Loss:  1666.1659837341308\n",
      "\n",
      "\n",
      "Epoch:  132\n",
      "  Training Loss:  1258.2632145182292\n",
      "  Validation Loss:  1658.5823696136474\n",
      "\n",
      "\n",
      "Epoch:  133\n",
      "  Training Loss:  1253.2041903940838\n",
      "  Validation Loss:  1653.4342505645752\n",
      "\n",
      "\n",
      "Epoch:  134\n",
      "  Training Loss:  1262.1371154530843\n",
      "  Validation Loss:  1640.3158686065674\n",
      "\n",
      "\n",
      "Epoch:  135\n",
      "  Training Loss:  1236.8462327067057\n",
      "  Validation Loss:  1712.7388019180298\n",
      "\n",
      "\n",
      "Epoch:  136\n",
      "  Training Loss:  1238.5075098164875\n",
      "  Validation Loss:  1643.1835634231568\n",
      "\n",
      "\n",
      "Epoch:  137\n",
      "  Training Loss:  1238.516887334188\n",
      "  Validation Loss:  1655.854857673645\n",
      "\n",
      "\n",
      "Epoch:  138\n",
      "  Training Loss:  1252.3960718536377\n",
      "  Validation Loss:  1626.5919757843017\n",
      "\n",
      "\n",
      "Epoch:  139\n",
      "  Training Loss:  1246.3842503102621\n",
      "  Validation Loss:  1650.7335930633544\n",
      "\n",
      "\n",
      "Epoch:  140\n",
      "  Training Loss:  1232.4943226114908\n",
      "  Validation Loss:  1650.136731414795\n",
      "\n",
      "\n",
      "Epoch:  141\n",
      "  Training Loss:  1224.098809814453\n",
      "  Validation Loss:  1667.8188378143311\n",
      "\n",
      "\n",
      "Epoch:  142\n",
      "  Training Loss:  1226.5839252726237\n",
      "  Validation Loss:  1676.6230951690675\n",
      "\n",
      "\n",
      "Epoch:  143\n",
      "  Training Loss:  1245.055739212036\n",
      "  Validation Loss:  1653.170372543335\n",
      "\n",
      "\n",
      "Epoch:  144\n",
      "  Training Loss:  1218.383530553182\n",
      "  Validation Loss:  1658.5731147766114\n",
      "\n",
      "\n",
      "Epoch:  145\n",
      "  Training Loss:  1214.1177683003743\n",
      "  Validation Loss:  1696.0908423995972\n",
      "\n",
      "\n",
      "Epoch:  146\n",
      "  Training Loss:  1215.74390431722\n",
      "  Validation Loss:  1677.5331021118163\n",
      "\n",
      "\n",
      "Epoch:  147\n",
      "  Training Loss:  1226.569441248576\n",
      "  Validation Loss:  1660.0516321182251\n",
      "\n",
      "\n",
      "Epoch:  148\n",
      "  Training Loss:  1222.1403231811523\n",
      "  Validation Loss:  1653.965272140503\n",
      "\n",
      "\n",
      "Epoch:  149\n",
      "  Training Loss:  1196.0969750213624\n",
      "  Validation Loss:  1632.1597328186035\n",
      "\n",
      "\n",
      "Epoch:  150\n",
      "  Training Loss:  1199.7921037546794\n",
      "  Validation Loss:  1689.1981623458862\n",
      "\n",
      "\n",
      "Epoch:  151\n",
      "  Training Loss:  1184.979333826701\n",
      "  Validation Loss:  1644.053187713623\n",
      "\n",
      "\n",
      "Epoch:  152\n",
      "  Training Loss:  1177.0581483713786\n",
      "  Validation Loss:  1665.8951895141602\n",
      "\n",
      "\n",
      "Epoch:  153\n",
      "  Training Loss:  1181.697100753784\n",
      "  Validation Loss:  1670.1054935455322\n",
      "\n",
      "\n",
      "Epoch:  154\n",
      "  Training Loss:  1197.2677835337322\n",
      "  Validation Loss:  1663.47229347229\n",
      "\n",
      "\n",
      "Epoch:  155\n",
      "  Training Loss:  1180.0425146738687\n",
      "  Validation Loss:  1679.087190322876\n",
      "\n",
      "\n",
      "Epoch:  156\n",
      "  Training Loss:  1179.337808380127\n",
      "  Validation Loss:  1645.4441444396973\n",
      "\n",
      "\n",
      "Epoch:  157\n",
      "  Training Loss:  1158.35560546875\n",
      "  Validation Loss:  1633.8058060455323\n",
      "\n",
      "\n",
      "Epoch:  158\n",
      "  Training Loss:  1157.6304370371502\n",
      "  Validation Loss:  1701.2302300643921\n",
      "\n",
      "\n",
      "Epoch:  159\n",
      "  Training Loss:  1185.5203749084474\n",
      "  Validation Loss:  1698.0279138946532\n",
      "\n",
      "\n",
      "Epoch:  160\n",
      "  Training Loss:  1214.2531177775065\n",
      "  Validation Loss:  1616.500544013977\n",
      "\n",
      "\n",
      "Epoch:  161\n",
      "  Training Loss:  1208.6335591634115\n",
      "  Validation Loss:  1668.6558478164673\n",
      "\n",
      "\n",
      "Epoch:  162\n",
      "  Training Loss:  1171.379264755249\n",
      "  Validation Loss:  1696.9828384017944\n",
      "\n",
      "\n",
      "Epoch:  163\n",
      "  Training Loss:  1156.8944724527994\n",
      "  Validation Loss:  1739.5826446151734\n",
      "\n",
      "\n",
      "Epoch:  164\n",
      "  Training Loss:  1156.6070965321858\n",
      "  Validation Loss:  1674.9104201507569\n",
      "\n",
      "\n",
      "Epoch:  165\n",
      "  Training Loss:  1146.0489995829264\n",
      "  Validation Loss:  1655.692465133667\n",
      "\n",
      "\n",
      "Epoch:  166\n",
      "  Training Loss:  1167.988179397583\n",
      "  Validation Loss:  1649.6186282348633\n",
      "\n",
      "\n",
      "Epoch:  167\n",
      "  Training Loss:  1144.5574289449055\n",
      "  Validation Loss:  1704.3608464050294\n",
      "\n",
      "\n",
      "Epoch:  168\n",
      "  Training Loss:  1142.839910888672\n",
      "  Validation Loss:  1650.2324807357788\n",
      "\n",
      "\n",
      "Epoch:  169\n",
      "  Training Loss:  1157.345069249471\n",
      "  Validation Loss:  1714.444394416809\n",
      "\n",
      "\n",
      "Epoch:  170\n",
      "  Training Loss:  1139.454275970459\n",
      "  Validation Loss:  1699.5165920639038\n",
      "\n",
      "\n",
      "Epoch:  171\n",
      "  Training Loss:  1144.0775265248617\n",
      "  Validation Loss:  1674.1118128204346\n",
      "\n",
      "\n",
      "Epoch:  172\n",
      "  Training Loss:  1118.5633862559\n",
      "  Validation Loss:  1662.87379737854\n",
      "\n",
      "\n",
      "Epoch:  173\n",
      "  Training Loss:  1095.418696416219\n",
      "  Validation Loss:  1691.276675643921\n",
      "\n",
      "\n",
      "Epoch:  174\n",
      "  Training Loss:  1090.635030110677\n",
      "  Validation Loss:  1724.7472186279297\n",
      "\n",
      "\n",
      "Epoch:  175\n",
      "  Training Loss:  1106.7116429392497\n",
      "  Validation Loss:  1697.1345977783203\n",
      "\n",
      "\n",
      "Epoch:  176\n",
      "  Training Loss:  1109.0501314290364\n",
      "  Validation Loss:  1693.077292175293\n",
      "\n",
      "\n",
      "Epoch:  177\n",
      "  Training Loss:  1108.1492681630452\n",
      "  Validation Loss:  1695.2455210113526\n",
      "\n",
      "\n",
      "Epoch:  178\n",
      "  Training Loss:  1103.243135172526\n",
      "  Validation Loss:  1679.5654681777953\n",
      "\n",
      "\n",
      "Epoch:  179\n",
      "  Training Loss:  1090.9527334086101\n",
      "  Validation Loss:  1656.964022064209\n",
      "\n",
      "\n",
      "Epoch:  180\n",
      "  Training Loss:  1082.9892793528238\n",
      "  Validation Loss:  1658.8682752227783\n",
      "\n",
      "\n",
      "Epoch:  181\n",
      "  Training Loss:  1060.2249925231934\n",
      "  Validation Loss:  1653.729591217041\n",
      "\n",
      "\n",
      "Epoch:  182\n",
      "  Training Loss:  1056.4671313476563\n",
      "  Validation Loss:  1717.3409859466553\n",
      "\n",
      "\n",
      "Epoch:  183\n",
      "  Training Loss:  1071.9271250152588\n",
      "  Validation Loss:  1749.768674621582\n",
      "\n",
      "\n",
      "Epoch:  184\n",
      "  Training Loss:  1077.385481007894\n",
      "  Validation Loss:  1709.3174183654785\n",
      "\n",
      "\n",
      "Epoch:  185\n",
      "  Training Loss:  1044.1071654256184\n",
      "  Validation Loss:  1688.9675904083251\n",
      "\n",
      "\n",
      "Epoch:  186\n",
      "  Training Loss:  1036.2953813171387\n",
      "  Validation Loss:  1688.1702724838258\n",
      "\n",
      "\n",
      "Epoch:  187\n",
      "  Training Loss:  1030.1039180501302\n",
      "  Validation Loss:  1728.6277185821534\n",
      "\n",
      "\n",
      "Epoch:  188\n",
      "  Training Loss:  1065.0544825744628\n",
      "  Validation Loss:  1683.807844619751\n",
      "\n",
      "\n",
      "Epoch:  189\n",
      "  Training Loss:  1040.4009917958576\n",
      "  Validation Loss:  1688.4518128967286\n",
      "\n",
      "\n",
      "Epoch:  190\n",
      "  Training Loss:  1031.2361838022869\n",
      "  Validation Loss:  1724.653632736206\n",
      "\n",
      "\n",
      "Epoch:  191\n",
      "  Training Loss:  1053.4066196187337\n",
      "  Validation Loss:  1781.638652191162\n",
      "\n",
      "\n",
      "Epoch:  192\n",
      "  Training Loss:  1068.655177892049\n",
      "  Validation Loss:  1664.9997385406493\n",
      "\n",
      "\n",
      "Epoch:  193\n",
      "  Training Loss:  1038.7513416544596\n",
      "  Validation Loss:  1704.1342658233643\n",
      "\n",
      "\n",
      "Epoch:  194\n",
      "  Training Loss:  1018.5271868133545\n",
      "  Validation Loss:  1789.512373046875\n",
      "\n",
      "\n",
      "Epoch:  195\n",
      "  Training Loss:  1011.6351965586344\n",
      "  Validation Loss:  1696.630252723694\n",
      "\n",
      "\n",
      "Epoch:  196\n",
      "  Training Loss:  1018.7953951263428\n",
      "  Validation Loss:  1697.2287499237061\n",
      "\n",
      "\n",
      "Epoch:  197\n",
      "  Training Loss:  1049.006387049357\n",
      "  Validation Loss:  1666.6393563079835\n",
      "\n",
      "\n",
      "Epoch:  198\n",
      "  Training Loss:  1012.9058763631185\n",
      "  Validation Loss:  1692.6004308700562\n",
      "\n",
      "\n",
      "Epoch:  199\n",
      "  Training Loss:  997.1693906656901\n",
      "  Validation Loss:  1771.5169963073731\n",
      "\n",
      "\n",
      "Epoch:  200\n",
      "  Training Loss:  1002.257398707072\n",
      "  Validation Loss:  1758.5393438339233\n",
      "\n",
      "\n",
      "Epoch:  201\n",
      "  Training Loss:  998.9758160654703\n",
      "  Validation Loss:  1650.6163593673707\n",
      "\n",
      "\n",
      "Epoch:  202\n",
      "  Training Loss:  989.4114409891764\n",
      "  Validation Loss:  1675.127088394165\n",
      "\n",
      "\n",
      "Epoch:  203\n",
      "  Training Loss:  995.5301454925537\n",
      "  Validation Loss:  1661.9452922821044\n",
      "\n",
      "\n",
      "Epoch:  204\n",
      "  Training Loss:  1011.8436722310385\n",
      "  Validation Loss:  1668.0251512145996\n",
      "\n",
      "\n",
      "Epoch:  205\n",
      "  Training Loss:  977.4845904286702\n",
      "  Validation Loss:  1686.731880607605\n",
      "\n",
      "\n",
      "Epoch:  206\n",
      "  Training Loss:  998.2475965372721\n",
      "  Validation Loss:  1693.7248651123048\n",
      "\n",
      "\n",
      "Epoch:  207\n",
      "  Training Loss:  975.497976582845\n",
      "  Validation Loss:  1665.0514427947999\n",
      "\n",
      "\n",
      "Epoch:  208\n",
      "  Training Loss:  960.0646988169352\n",
      "  Validation Loss:  1698.657586917877\n",
      "\n",
      "\n",
      "Epoch:  209\n",
      "  Training Loss:  943.8820421346029\n",
      "  Validation Loss:  1693.755076713562\n",
      "\n",
      "\n",
      "Epoch:  210\n",
      "  Training Loss:  942.6817320251465\n",
      "  Validation Loss:  1694.7408011627197\n",
      "\n",
      "\n",
      "Epoch:  211\n",
      "  Training Loss:  1002.2751225026449\n",
      "  Validation Loss:  1718.9322762298584\n",
      "\n",
      "\n",
      "Epoch:  212\n",
      "  Training Loss:  959.7467269388835\n",
      "  Validation Loss:  1731.417304763794\n",
      "\n",
      "\n",
      "Epoch:  213\n",
      "  Training Loss:  953.1017332967123\n",
      "  Validation Loss:  1684.9535927963257\n",
      "\n",
      "\n",
      "Epoch:  214\n",
      "  Training Loss:  932.8634454345703\n",
      "  Validation Loss:  1665.0929585647582\n",
      "\n",
      "\n",
      "Epoch:  215\n",
      "  Training Loss:  986.9882858530681\n",
      "  Validation Loss:  1715.0485808563233\n",
      "\n",
      "\n",
      "Epoch:  216\n",
      "  Training Loss:  995.4639038848877\n",
      "  Validation Loss:  1652.133314666748\n",
      "\n",
      "\n",
      "Epoch:  217\n",
      "  Training Loss:  974.4935121154786\n",
      "  Validation Loss:  1695.5809156799316\n",
      "\n",
      "\n",
      "Epoch:  218\n",
      "  Training Loss:  975.0105290476481\n",
      "  Validation Loss:  1698.6717014694214\n",
      "\n",
      "\n",
      "Epoch:  219\n",
      "  Training Loss:  948.5342468770345\n",
      "  Validation Loss:  1700.8450854873656\n",
      "\n",
      "\n",
      "Epoch:  220\n",
      "  Training Loss:  934.0701683044433\n",
      "  Validation Loss:  1684.1326307678223\n",
      "\n",
      "\n",
      "Epoch:  221\n",
      "  Training Loss:  955.7446858469646\n",
      "  Validation Loss:  1672.7760820007325\n",
      "\n",
      "\n",
      "Epoch:  222\n",
      "  Training Loss:  938.9195140329997\n",
      "  Validation Loss:  1736.9991342926025\n",
      "\n",
      "\n",
      "Epoch:  223\n",
      "  Training Loss:  908.6275830841064\n",
      "  Validation Loss:  1694.6098658370972\n",
      "\n",
      "\n",
      "Epoch:  224\n",
      "  Training Loss:  902.6077109781901\n",
      "  Validation Loss:  1714.9125610351562\n",
      "\n",
      "\n",
      "Epoch:  225\n",
      "  Training Loss:  896.9870738220214\n",
      "  Validation Loss:  1735.5571377944946\n",
      "\n",
      "\n",
      "Epoch:  226\n",
      "  Training Loss:  934.4401188913981\n",
      "  Validation Loss:  1710.741349887848\n",
      "\n",
      "\n",
      "Epoch:  227\n",
      "  Training Loss:  916.6363418070475\n",
      "  Validation Loss:  1712.9115800094605\n",
      "\n",
      "\n",
      "Epoch:  228\n",
      "  Training Loss:  889.3683638000489\n",
      "  Validation Loss:  1730.0022873687744\n",
      "\n",
      "\n",
      "Epoch:  229\n",
      "  Training Loss:  882.5317464192708\n",
      "  Validation Loss:  1709.2833108520508\n",
      "\n",
      "\n",
      "Epoch:  230\n",
      "  Training Loss:  933.0113627115885\n",
      "  Validation Loss:  1706.7092255401612\n",
      "\n",
      "\n",
      "Epoch:  231\n",
      "  Training Loss:  884.3041986846924\n",
      "  Validation Loss:  1677.939220046997\n",
      "\n",
      "\n",
      "Epoch:  232\n",
      "  Training Loss:  865.4022100321451\n",
      "  Validation Loss:  1704.9773957824707\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "butler.train_attn_model(train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc503e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
