{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e577f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "\n",
    "\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.models as models\n",
    "from handler import geozoom_handler\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import PIL\n",
    "\n",
    "\n",
    "from sa import *\n",
    "from fc_net import *\n",
    "from helpers import *\n",
    "from utils2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e892b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.sample(range(0, len(image_names)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "414822af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 municipalities.\n"
     ]
    }
   ],
   "source": [
    "# image_names = get_png_names(\"../pooling/data/MEX2/\")#[100:105]\n",
    "image_names = get_png_names(\"../pooling/data/MEX2/\")\n",
    "image_indices = random.sample(range(0, len(image_names)), 300)\n",
    "image_names = [image_names[i] for i in range(len(image_names)) if i in image_indices]\n",
    "y = get_migrants(\"../pooling/data/migration_data.json\" , image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4a3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in image_names:\n",
    "#     print(load_inputs(i).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8583389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18 = models.resnet18().to(device)\n",
    "attn_model = attnNet(resnet = resnet18).to(device)\n",
    "lr = .001\n",
    "criterion = torch.nn.L1Loss(reduction = 'mean')\n",
    "attn_optimizer = torch.optim.Adam(attn_model.parameters(), lr = lr)\n",
    "\n",
    "fc_model = fc_net(resnet = resnet18).to(device)\n",
    "fc_optimizer = torch.optim.Adam(fc_model.parameters(), lr = .01)\n",
    "\n",
    "butler = geozoom_handler(attn_model, \n",
    "                         fc_model, device, \n",
    "                         criterion, \n",
    "                         attn_optimizer, \n",
    "                         fc_optimizer, \n",
    "                         num_thresholds = 10,\n",
    "                         reduction_percent = .90,\n",
    "                         convergence_dims = (358, 284),\n",
    "                         plot = False, \n",
    "                         v = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a47a0a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (query_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (key_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (value_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_model.sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07ca59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "SPLIT = .60\n",
    "\n",
    "train_dl, val_dl = butler.prep_attn_data(image_names, y, SPLIT, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5e8fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,o in val_dl:\n",
    "#     print(load_inputs(i[0]).shape, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ad57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "  Training Loss:  1425.1291727807786\n",
      "  Validation Loss:  1737.4661228815714\n",
      "Loss thresholds for training:  [0.0, 173.74661228815714, 347.4932245763143, 521.2398368644714, 694.9864491526286, 868.7330614407857, 1042.4796737289428, 1216.2262860171, 1389.972898305257, 1563.7195105934143]\n",
      "Starting from threshold:  9  with value:  1563.7195105934143\n",
      "\n",
      "\n",
      "Epoch:  1\n",
      "  Training Loss:  1367.489933649699\n",
      "  Validation Loss:  1693.103741836548\n",
      "\n",
      "\n",
      "Epoch:  2\n",
      "  Training Loss:  1344.431157599555\n",
      "  Validation Loss:  1669.5068967183431\n",
      "\n",
      "\n",
      "Epoch:  3\n",
      "  Training Loss:  1335.6826685587564\n",
      "  Validation Loss:  1664.6083841959635\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "  Training Loss:  1329.4608574761285\n",
      "  Validation Loss:  1655.1432375590007\n",
      "\n",
      "\n",
      "Epoch:  5\n",
      "  Training Loss:  1322.4259048461913\n",
      "  Validation Loss:  1643.8307373046875\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "  Training Loss:  1311.729699198405\n",
      "  Validation Loss:  1631.387195332845\n",
      "\n",
      "\n",
      "Epoch:  7\n",
      "  Training Loss:  1307.433081902398\n",
      "  Validation Loss:  1631.795947011312\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "  Training Loss:  1304.264815097385\n",
      "  Validation Loss:  1625.0173072814941\n",
      "\n",
      "\n",
      "Epoch:  9\n",
      "  Training Loss:  1296.0630071004232\n",
      "  Validation Loss:  1618.6766716003417\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "  Training Loss:  1291.9961498684354\n",
      "  Validation Loss:  1607.7812052408854\n",
      "\n",
      "\n",
      "Epoch:  11\n",
      "  Training Loss:  1291.251505025228\n",
      "  Validation Loss:  1610.7670677185058\n",
      "\n",
      "\n",
      "Epoch:  12\n",
      "  Training Loss:  1290.9506010267469\n",
      "  Validation Loss:  1603.6508687337239\n",
      "\n",
      "\n",
      "Epoch:  13\n",
      "  Training Loss:  1286.1571643405491\n",
      "  Validation Loss:  1603.1753522237143\n",
      "\n",
      "\n",
      "Epoch:  14\n",
      "  Training Loss:  1289.4923268636069\n",
      "  Validation Loss:  1603.613133239746\n",
      "\n",
      "\n",
      "Epoch:  15\n",
      "  Training Loss:  1284.5776206122505\n",
      "  Validation Loss:  1604.6755332946777\n",
      "\n",
      "\n",
      "Epoch:  16\n",
      "  Training Loss:  1282.8449395073785\n",
      "  Validation Loss:  1605.517881266276\n",
      "\n",
      "\n",
      "Epoch:  17\n",
      "  Training Loss:  1279.2640362209745\n",
      "  Validation Loss:  1601.0818143208821\n",
      "\n",
      "\n",
      "Epoch:  18\n",
      "  Training Loss:  1277.181374613444\n",
      "  Validation Loss:  1608.4793917338054\n",
      "\n",
      "\n",
      "Epoch:  19\n",
      "  Training Loss:  1276.1726440429688\n",
      "  Validation Loss:  1595.731520843506\n",
      "\n",
      "\n",
      "Epoch:  20\n",
      "  Training Loss:  1272.7349005805122\n",
      "  Validation Loss:  1598.1201059977213\n",
      "\n",
      "\n",
      "Epoch:  21\n",
      "  Training Loss:  1269.9918969048395\n",
      "  Validation Loss:  1595.441689046224\n",
      "\n",
      "\n",
      "Epoch:  22\n",
      "  Training Loss:  1274.8368209838868\n",
      "  Validation Loss:  1598.8865203857422\n",
      "\n",
      "\n",
      "Epoch:  23\n",
      "  Training Loss:  1271.1295305040148\n",
      "  Validation Loss:  1597.5829737345377\n",
      "\n",
      "\n",
      "Epoch:  24\n",
      "  Training Loss:  1264.638605923123\n",
      "  Validation Loss:  1604.1440241495768\n",
      "\n",
      "\n",
      "Epoch:  25\n",
      "  Training Loss:  1270.5718187120226\n",
      "  Validation Loss:  1594.087479909261\n",
      "\n",
      "\n",
      "Epoch:  26\n",
      "  Training Loss:  1270.9589570787218\n",
      "  Validation Loss:  1598.1521489461263\n",
      "\n",
      "\n",
      "Epoch:  27\n",
      "  Training Loss:  1265.349324883355\n",
      "  Validation Loss:  1591.2509167989094\n",
      "\n",
      "\n",
      "Epoch:  28\n",
      "  Training Loss:  1264.5533233642577\n",
      "  Validation Loss:  1591.178482055664\n",
      "\n",
      "\n",
      "Epoch:  29\n",
      "  Training Loss:  1259.6183346218534\n",
      "  Validation Loss:  1591.9136133829752\n",
      "\n",
      "\n",
      "Epoch:  30\n",
      "  Training Loss:  1263.2762239244248\n",
      "  Validation Loss:  1589.1078404744467\n",
      "\n",
      "\n",
      "Epoch:  31\n",
      "  Training Loss:  1256.4247578938803\n",
      "  Validation Loss:  1589.9737024943033\n",
      "\n",
      "\n",
      "Epoch:  32\n",
      "  Training Loss:  1253.7146408081055\n",
      "  Validation Loss:  1581.9993797302245\n",
      "\n",
      "\n",
      "Epoch:  33\n",
      "  Training Loss:  1250.471284315321\n",
      "  Validation Loss:  1581.1159662882487\n",
      "\n",
      "\n",
      "Epoch:  34\n",
      "  Training Loss:  1244.9592968410916\n",
      "  Validation Loss:  1586.0046531677247\n",
      "\n",
      "\n",
      "Epoch:  35\n",
      "  Training Loss:  1244.5558057996961\n",
      "  Validation Loss:  1578.975634765625\n",
      "\n",
      "\n",
      "Epoch:  36\n",
      "  Training Loss:  1234.7749821980794\n",
      "  Validation Loss:  1577.690591430664\n",
      "\n",
      "\n",
      "Epoch:  37\n",
      "  Training Loss:  1231.4130398220486\n",
      "  Validation Loss:  1570.757411956787\n",
      "\n",
      "\n",
      "Epoch:  38\n",
      "  Training Loss:  1220.932909986708\n",
      "  Validation Loss:  1568.6934191385906\n",
      "\n",
      "\n",
      "Epoch:  39\n",
      "  Training Loss:  1218.7762573242187\n",
      "  Validation Loss:  1566.6592781066895\n",
      "\n",
      "\n",
      "Epoch:  40\n",
      "  Training Loss:  1206.9776958889431\n",
      "  Validation Loss:  1569.2338829040527\n",
      "\n",
      "\n",
      "Epoch:  41\n",
      "  Training Loss:  1198.3260450575087\n",
      "  Validation Loss:  1563.6646840413412\n",
      "  Moving to threshold:  8   |  Next loss benchmark:  1389.972898305257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/nn/functional.py:3328: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing random weights.\n",
      "\n",
      "\n",
      "Epoch:  42\n",
      "  Training Loss:  1217.6345035129123\n",
      "  Validation Loss:  1571.5354062398276\n",
      "\n",
      "\n",
      "Epoch:  43\n",
      "  Training Loss:  1202.7359585232205\n",
      "  Validation Loss:  1572.0446230570476\n",
      "\n",
      "\n",
      "Epoch:  44\n",
      "  Training Loss:  1189.9647952609591\n",
      "  Validation Loss:  1573.3786153157553\n",
      "\n",
      "\n",
      "Epoch:  45\n",
      "  Training Loss:  1170.2993370903862\n",
      "  Validation Loss:  1569.170051320394\n",
      "\n",
      "\n",
      "Epoch:  46\n",
      "  Training Loss:  1155.550681898329\n",
      "  Validation Loss:  1581.2590558369955\n",
      "\n",
      "\n",
      "Epoch:  47\n",
      "  Training Loss:  1143.3155339558919\n",
      "  Validation Loss:  1564.398506673177\n",
      "\n",
      "\n",
      "Epoch:  48\n",
      "  Training Loss:  1158.104810587565\n",
      "  Validation Loss:  1561.1119468688964\n",
      "\n",
      "\n",
      "Epoch:  49\n",
      "  Training Loss:  1117.9909598456488\n",
      "  Validation Loss:  1570.2025967915854\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "butler.train_attn_model(train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2863ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_outside(cur_image, output):\n",
    "\n",
    "    model = butler.attn_model\n",
    "    keys = list(butler.threshold_weights.keys())\n",
    "    \n",
    "    print(\"Keys: \", keys)\n",
    "\n",
    "    # If there are acutally weights in the threshold dictionary (i.e. if the training data has been clipped at least once)...\n",
    "    if len(keys) > 0:\n",
    "\n",
    "        # For the weights equivalent to each clip...\n",
    "        for k in butler.threshold_weights.keys():\n",
    "\n",
    "            # Load the weights that coorespond to that clip\n",
    "            model.load_state_dict(butler.threshold_weights[k])\n",
    "\n",
    "            # and set phasers to stun...\n",
    "            model.eval()\n",
    "\n",
    "            # If it is the last key in the dictionary...\n",
    "            if k == keys[-1]:\n",
    "                \n",
    "                print(\"Last key\")\n",
    "                \n",
    "                print(\"Shape before clip: \", cur_image.shape)\n",
    "\n",
    "                # Do all of the things to get the attention map and clip the image\n",
    "                IM_SIZE = (cur_image.shape[2], cur_image.shape[3])\n",
    "                gradcam, attn_heatmap = get_gradcam(model, IM_SIZE, cur_image.cuda(), target_layer = butler.model.sa) \n",
    "                cur_image, new_dims = butler.clip_input(cur_image, attn_heatmap)   \n",
    "                \n",
    "                print(\"Shape after clip: \", cur_image.shape)\n",
    "\n",
    "                # Now that you have the image according to all of the past clips, run it through the current state of the model (mid-threshold weights)\n",
    "                butler.model.eval()\n",
    "                y_pred = butler.model(cur_image.cuda())\n",
    "                print(\"PRED: \", y_pred.item())\n",
    "                loss = butler.criterion(y_pred, output.view(-1,1).to(butler.device))\n",
    "                print(\"LOSS: \", loss.item())\n",
    "#                 butler.running_val_loss += l oss.item() \n",
    "\n",
    "            # If it's not the last key in the dictionary, clip it to the k weights and send it back through again\n",
    "            else:\n",
    "\n",
    "                IM_SIZE = (cur_image.shape[2], cur_image.shape[3])\n",
    "                gradcam, attn_heatmap = get_gradcam(model, IM_SIZE, cur_image.cuda(), target_layer = butler.model.sa) \n",
    "                cur_image, new_dims = butler.clip_input(cur_image, attn_heatmap)    \n",
    "\n",
    "    # If there are no threshold weights, just run it through the model as is\n",
    "    else:\n",
    "\n",
    "        butler.model.eval()\n",
    "        y_pred = butler.model(cur_image.cuda())\n",
    "        print(\"PRED: \", y_pred.item())\n",
    "        loss = butler.criterion(y_pred, output.view(-1,1).to(butler.device))\n",
    "        print(\"LOSS: \", loss.item())\n",
    "#         butler.running_val_loss += loss.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62fd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78faae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc95f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28d21e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 1325, 1658])\n",
      "Shape after clip:  torch.Size([1, 3, 1192, 1492])\n",
      "PRED:  310.58905029296875\n",
      "LOSS:  915.4109497070312\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 539, 579])\n",
      "Shape after clip:  torch.Size([1, 3, 485, 521])\n",
      "PRED:  338.44354248046875\n",
      "LOSS:  86.44354248046875\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 462, 699])\n",
      "Shape after clip:  torch.Size([1, 3, 415, 629])\n",
      "PRED:  286.9433288574219\n",
      "LOSS:  259.0566711425781\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 355, 308])\n",
      "Shape after clip:  torch.Size([1, 3, 224, 224])\n",
      "PRED:  369.9192199707031\n",
      "LOSS:  59.919219970703125\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 1909, 1751])\n",
      "Shape after clip:  torch.Size([1, 3, 1718, 1575])\n",
      "PRED:  276.0505676269531\n",
      "LOSS:  549.949462890625\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 144, 210])\n",
      "Shape after clip:  torch.Size([1, 3, 144, 14])\n",
      "PRED:  272.150390625\n",
      "LOSS:  220.150390625\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 392, 689])\n",
      "Shape after clip:  torch.Size([1, 3, 224, 224])\n",
      "PRED:  291.8940734863281\n",
      "LOSS:  14.105926513671875\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 354, 344])\n",
      "Shape after clip:  torch.Size([1, 3, 224, 224])\n",
      "PRED:  397.6798095703125\n",
      "LOSS:  243.6798095703125\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 395, 467])\n",
      "Shape after clip:  torch.Size([1, 3, 224, 224])\n",
      "PRED:  299.3555908203125\n",
      "LOSS:  1116.6444091796875\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 345, 441])\n",
      "Shape after clip:  torch.Size([1, 3, 224, 224])\n",
      "PRED:  295.99652099609375\n",
      "LOSS:  1928.00341796875\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 2774, 2238])\n",
      "Shape after clip:  torch.Size([1, 3, 2496, 2014])\n",
      "PRED:  309.51910400390625\n",
      "LOSS:  181.51910400390625\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 673, 1306])\n",
      "Shape after clip:  torch.Size([1, 3, 605, 1175])\n",
      "PRED:  234.32882690429688\n",
      "LOSS:  113.32882690429688\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 987, 1687])\n",
      "Shape after clip:  torch.Size([1, 3, 888, 1518])\n",
      "PRED:  205.2133331298828\n",
      "LOSS:  930.7866821289062\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 1681, 1776])\n",
      "Shape after clip:  torch.Size([1, 3, 1512, 1598])\n",
      "PRED:  260.8000793457031\n",
      "LOSS:  198.80007934570312\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 385, 278])\n",
      "Shape after clip:  torch.Size([1, 3, 224, 224])\n",
      "PRED:  348.7381286621094\n",
      "LOSS:  323.2618713378906\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 2535, 1630])\n",
      "Shape after clip:  torch.Size([1, 3, 2281, 1467])\n",
      "PRED:  259.4169006347656\n",
      "LOSS:  1023.5831298828125\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 1067, 736])\n",
      "Shape after clip:  torch.Size([1, 3, 960, 662])\n",
      "PRED:  206.1179656982422\n",
      "LOSS:  96.11796569824219\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 427, 561])\n",
      "Shape after clip:  torch.Size([1, 3, 384, 504])\n",
      "PRED:  345.5749206542969\n",
      "LOSS:  584.425048828125\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 476, 619])\n",
      "Shape after clip:  torch.Size([1, 3, 428, 557])\n",
      "PRED:  251.390869140625\n",
      "LOSS:  284.609130859375\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 1504, 1247])\n",
      "Shape after clip:  torch.Size([1, 3, 1353, 1122])\n",
      "PRED:  381.6424255371094\n",
      "LOSS:  2484.357666015625\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 1591, 1722])\n",
      "Shape after clip:  torch.Size([1, 3, 1431, 1549])\n",
      "PRED:  309.2651672363281\n",
      "LOSS:  187.26516723632812\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 150, 223])\n",
      "Shape after clip:  torch.Size([1, 3, 150, 1])\n",
      "PRED:  353.7044677734375\n",
      "LOSS:  84.2955322265625\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 1824, 1414])\n",
      "Shape after clip:  torch.Size([1, 3, 1641, 1272])\n",
      "PRED:  234.80152893066406\n",
      "LOSS:  2475.198486328125\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 423, 628])\n",
      "Shape after clip:  torch.Size([1, 3, 380, 565])\n",
      "PRED:  248.57530212402344\n",
      "LOSS:  1103.4246826171875\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 229, 168])\n",
      "Shape after clip:  torch.Size([1, 3, 224, 168])\n",
      "PRED:  289.80584716796875\n",
      "LOSS:  289.80584716796875\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 152, 267])\n",
      "Shape after clip:  torch.Size([1, 3, 152, 224])\n",
      "PRED:  328.6294250488281\n",
      "LOSS:  14.629425048828125\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 577, 642])\n",
      "Shape after clip:  torch.Size([1, 3, 519, 577])\n",
      "PRED:  320.5124816894531\n",
      "LOSS:  1548.487548828125\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 1362, 2274])\n",
      "Shape after clip:  torch.Size([1, 3, 1225, 2046])\n",
      "PRED:  332.6629943847656\n",
      "LOSS:  7687.3369140625\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 1962, 2691])\n",
      "Shape after clip:  torch.Size([1, 3, 1765, 2421])\n",
      "PRED:  262.3731689453125\n",
      "LOSS:  176.3731689453125\n",
      "Keys:  [9]\n",
      "Last key\n",
      "Shape before clip:  torch.Size([1, 3, 1893, 1497])\n",
      "Shape after clip:  torch.Size([1, 3, 1703, 1347])\n",
      "PRED:  256.1780700683594\n",
      "LOSS:  5869.82177734375\n"
     ]
    }
   ],
   "source": [
    "for i, o in val_dl:\n",
    "    inp = butler.prep_input(i)\n",
    "    val_outside(inp, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b680f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butler.threshold_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cb4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c641c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4b04f82d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3bd8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.image_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c0b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.threshold_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84149b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,o in train_dl:\n",
    "    print(load_inputs(i[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc936ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "641 * .70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "263 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ccbd756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, o in train_dl:\n",
    "    \n",
    "    muni_id = i[0].split(\"/\")[4]\n",
    "    \n",
    "    plt.imshow(butler.prep_input(i).detach().cpu()[0].permute(1,2,0))\n",
    "    plt.title(butler.image_sizes[muni_id])\n",
    "    plt.savefig(f\"test{muni_id}.png\")\n",
    "    \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62a2826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in range(50):\n",
    "# while handler.threshold_index > 0:\n",
    "\n",
    "#     for impath, output in train_dl:\n",
    "\n",
    "#         # Prep the input and pass it to the trainer (this could easily be done in one step eventually if ya want)\n",
    "#         input = handler.prep_input(impath)\n",
    "#         handler.train(input, output)\n",
    "    \n",
    "#     handler.end_epoch(train_dl, val_dl = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca5f492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'484002003': (0, 224, 0, 224),\n",
       " '484016045': (0, 224, 0, 224),\n",
       " '484020489': (0, 224, 0, 224),\n",
       " '484020254': (0, 224, 0, 224)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler.image_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bfbb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(butler.threshold_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70776376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, o in val_dl:\n",
    "#     print(i, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7622d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_validation(attn_model, fc_model, input, weights_dict, plot = False):\n",
    "    \n",
    "#     muni_id = input[0].split(\"/\")[4]\n",
    "#     cur_image = load_inputs(input[0])    \n",
    "    \n",
    "#     for k in weights_dict.keys():\n",
    "                \n",
    "#         if k != 'fc':\n",
    "            \n",
    "#             model = attn_model\n",
    "#             model.load_state_dict(weights_dict[k])\n",
    "#             model.eval()\n",
    "#             IM_SIZE = (cur_image.shape[2], cur_image.shape[3])\n",
    "#             gradcam, attn_heatmap = get_gradcam(model, IM_SIZE, cur_image.cuda()) \n",
    "#             cur_image, new_dims = butler.clip_input(cur_image, attn_heatmap)\n",
    "            \n",
    "#             if plot == True:\n",
    "            \n",
    "#                 plot_gradcam(gradcam)\n",
    "#                 plt.savefig(f\"threshold{k}_muni{muni_id}.png\")\n",
    "#                 plt.clf()\n",
    "            \n",
    "        \n",
    "#         if k == 'fc':\n",
    "            \n",
    "#             model = fc_model\n",
    "#             model.load_state_dict(weights_dict[k])\n",
    "#             model.eval()\n",
    "#             return model(cur_image.cuda()).item()\n",
    "            \n",
    "\n",
    "        \n",
    "# for i, o in train_dl:\n",
    "\n",
    "#     print(round(run_validation(attn_model, fc_model, i, butler.threshold_weights), 2), \"     \", o.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61308046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
